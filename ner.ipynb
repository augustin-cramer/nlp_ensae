{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook contient le code utilisé pour produire les résultats de la partie 3 du rapport."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import json\n",
    "import yaml\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForTokenClassification, AutoModelForTokenClassification, TrainingArguments, Trainer, AutoTokenizer, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = yaml.safe_load(open(\"data/tokens.yml\", \"r\"))\n",
    "entities = json.load(open(\"data/entities.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = [v[\"start\"] for v in tags.values()]\n",
    "\n",
    "prefix_beginning = \"B-\"\n",
    "prefix_inside = \"I-\"\n",
    "labels_list = [prefix + tag for tag in all_tags for prefix in [prefix_beginning, prefix_inside]]\n",
    "label2id = {label: i for i, label in enumerate(labels_list)}\n",
    "id2label = {v: k for k,v in label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_to_list(lines):\n",
    "    return lines.split(\"\\n\")\n",
    "\n",
    "def line_to_infos(line):\n",
    "    all_tags_str = \"\".join(all_tags)\n",
    "    pattern = rf\"[{all_tags_str}].*?(?=(?:[{all_tags_str}])|$)\"\n",
    "    infos = {}\n",
    "    infos_in_lines = re.findall(pattern, line)\n",
    "    for info in infos_in_lines:\n",
    "        token = info[0]\n",
    "        text = info[1:].strip()\n",
    "        infos[token]=text\n",
    "    return infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infos_to_ds(infos):\n",
    "    tokens = []\n",
    "    ner_tags = []\n",
    "    for tag, text in infos.items():\n",
    "        splitted_text = text.split(\" \")\n",
    "        tags = [label2id[prefix_beginning + tag]] + [label2id[prefix_inside + tag]] * (len(splitted_text)-1)\n",
    "        tokens.extend(splitted_text)\n",
    "        ner_tags.extend(tags)\n",
    "    return tokens, ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_dict(entities):\n",
    "    lines_data = []\n",
    "    for lines in entities.values():\n",
    "        lines_data.extend(lines_to_list(lines))\n",
    "        \n",
    "    data = []\n",
    "    for line_data in lines_data:\n",
    "        data.append(line_to_infos(line_data))\n",
    "    \n",
    "    ds_dict = {\n",
    "        \"ner_tags\":[],\n",
    "        \"tokens\": []\n",
    "        }\n",
    "    \n",
    "    for infos in data:\n",
    "        tokens, ner_tags = infos_to_ds(infos)\n",
    "        ds_dict[\"tokens\"].append(tokens)\n",
    "        ds_dict[\"ner_tags\"].append(ner_tags)\n",
    "        \n",
    "    return ds_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(create_dataset_dict(entities)).train_test_split(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choix du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "# model_checkpoint = \"almanach/camembert-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenisation des séquences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549c848fa47b49a6bf7233a44eeda4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93d48ea4afbd4efaae6277f0719799ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5090 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du processus d'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement du modèle préentraîné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=len(labels_list), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/augustincramer/Desktop/vscode_projects/nlp/projet/venv/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c46f86918334fd79a4c659d60c1cad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.53, 'grad_norm': 1.6168144941329956, 'learning_rate': 1.607227022780833e-05, 'epoch': 0.39}\n",
      "{'loss': 0.1467, 'grad_norm': 2.387028694152832, 'learning_rate': 1.2144540455616654e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725ffec0d9e34e07b056a50320ac0b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_Ⓐ': {'precision': 0.7975579975579976, 'recall': 0.99633923123856, 'f1': 0.8859351688593518, 'number': 3278}, 'eval_Ⓑ': {'precision': 0.9958649207443143, 'recall': 0.9684986595174263, 'f1': 0.9819911654774041, 'number': 1492}, 'eval_Ⓒ': {'precision': 0.9900190114068441, 'recall': 0.9701909641360037, 'f1': 0.980004704775347, 'number': 2147}, 'eval_Ⓔ': {'precision': 0.8379254457050244, 'recall': 0.8792517006802721, 'f1': 0.858091286307054, 'number': 588}, 'eval_Ⓕ': {'precision': 0.9917559769167353, 'recall': 0.9664591283390239, 'f1': 0.9789441562404638, 'number': 4979}, 'eval_Ⓗ': {'precision': 0.9380488962734393, 'recall': 0.9459071325993298, 'f1': 0.9419616255511858, 'number': 4178}, 'eval_Ⓘ': {'precision': 0.9464954521134297, 'recall': 0.9459893048128343, 'f1': 0.946242310778283, 'number': 1870}, 'eval_Ⓚ': {'precision': 0.9783533049864708, 'recall': 0.9641904761904762, 'f1': 0.9712202609363008, 'number': 2625}, 'eval_Ⓛ': {'precision': 0.4411764705882353, 'recall': 0.22900763358778625, 'f1': 0.30150753768844224, 'number': 131}, 'eval_Ⓜ': {'precision': 0.9574260643483913, 'recall': 0.9197627224477053, 'f1': 0.9382165605095542, 'number': 3203}, 'eval_Ⓞ': {'precision': 0.9605883985567583, 'recall': 0.8982610952504542, 'f1': 0.9283798283261803, 'number': 3853}, 'eval_Ⓟ': {'precision': 0.7638347622759158, 'recall': 0.8852755194218609, 'f1': 0.8200836820083681, 'number': 1107}, 'eval_overall_precision': 0.9320232066803045, 'eval_overall_recall': 0.9436691453600896, 'eval_overall_f1': 0.9378100219335245, 'eval_overall_accuracy': 0.9443600562587904, 'eval_runtime': 11.2772, 'eval_samples_per_second': 451.354, 'eval_steps_per_second': 28.287, 'epoch': 1.0}\n",
      "{'loss': 0.1104, 'grad_norm': 0.9779603481292725, 'learning_rate': 8.21681068342498e-06, 'epoch': 1.18}\n",
      "{'loss': 0.0898, 'grad_norm': 0.8732245564460754, 'learning_rate': 4.289080911233308e-06, 'epoch': 1.57}\n",
      "{'loss': 0.0863, 'grad_norm': 1.7877209186553955, 'learning_rate': 3.6135113904163394e-07, 'epoch': 1.96}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c119843e73848fa99c18e1e2443bcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_Ⓐ': {'precision': 0.7971652003910068, 'recall': 0.9951189749847468, 'f1': 0.8852103120759838, 'number': 3278}, 'eval_Ⓑ': {'precision': 0.9958649207443143, 'recall': 0.9684986595174263, 'f1': 0.9819911654774041, 'number': 1492}, 'eval_Ⓒ': {'precision': 0.9918738049713193, 'recall': 0.9664648346530041, 'f1': 0.9790044821891956, 'number': 2147}, 'eval_Ⓔ': {'precision': 0.9306759098786829, 'recall': 0.9132653061224489, 'f1': 0.9218884120171674, 'number': 588}, 'eval_Ⓕ': {'precision': 0.98828125, 'recall': 0.9654549106246234, 'f1': 0.9767347353449152, 'number': 4979}, 'eval_Ⓗ': {'precision': 0.946875, 'recall': 0.9427955959789373, 'f1': 0.9448308946989685, 'number': 4178}, 'eval_Ⓘ': {'precision': 0.9560737527114967, 'recall': 0.9427807486631016, 'f1': 0.9493807215939687, 'number': 1870}, 'eval_Ⓚ': {'precision': 0.9859649122807017, 'recall': 0.9634285714285714, 'f1': 0.9745664739884392, 'number': 2625}, 'eval_Ⓛ': {'precision': 0.6435643564356436, 'recall': 0.4961832061068702, 'f1': 0.560344827586207, 'number': 131}, 'eval_Ⓜ': {'precision': 0.9560192616372392, 'recall': 0.9297533562285357, 'f1': 0.9427033871478315, 'number': 3203}, 'eval_Ⓞ': {'precision': 0.9581728123280132, 'recall': 0.9037113937191799, 'f1': 0.9301455856818486, 'number': 3853}, 'eval_Ⓟ': {'precision': 0.7775061124694377, 'recall': 0.8617886178861789, 'f1': 0.8174807197943446, 'number': 1107}, 'eval_overall_precision': 0.9365453199650091, 'eval_overall_recall': 0.9451631523547588, 'eval_overall_f1': 0.9408345022222974, 'eval_overall_accuracy': 0.9461322081575246, 'eval_runtime': 10.453, 'eval_samples_per_second': 486.942, 'eval_steps_per_second': 30.518, 'epoch': 2.0}\n",
      "{'train_runtime': 475.1057, 'train_samples_per_second': 85.699, 'train_steps_per_second': 5.359, 'train_loss': 0.19089205341833554, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2546, training_loss=0.19089205341833554, metrics={'train_runtime': 475.1057, 'train_samples_per_second': 85.699, 'train_steps_per_second': 5.359, 'train_loss': 0.19089205341833554, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"nlp_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédictions du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['samuel', 'louis', 'id', '##em', 'fi', '##ls', '1927', 'francais', 'id', '##em']\n",
      "['B-Ⓞ', 'B-Ⓕ', 'B-Ⓜ', 'B-Ⓗ', 'B-Ⓑ', 'B-Ⓚ', 'B-Ⓘ']\n"
     ]
    }
   ],
   "source": [
    "text = \" \".join(tokenized_ds[\"test\"][0][\"tokens\"])\n",
    "print(tokenizer.tokenize(text, add_special_tokens=False))\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-Ⓞ',\n",
       "  'score': 0.9934668,\n",
       "  'index': 1,\n",
       "  'word': 'samuel',\n",
       "  'start': 0,\n",
       "  'end': 6},\n",
       " {'entity': 'B-Ⓕ',\n",
       "  'score': 0.99295926,\n",
       "  'index': 2,\n",
       "  'word': 'louis',\n",
       "  'start': 7,\n",
       "  'end': 12},\n",
       " {'entity': 'B-Ⓜ',\n",
       "  'score': 0.9969694,\n",
       "  'index': 3,\n",
       "  'word': 'id',\n",
       "  'start': 13,\n",
       "  'end': 15},\n",
       " {'entity': 'I-Ⓜ',\n",
       "  'score': 0.80385107,\n",
       "  'index': 4,\n",
       "  'word': '##em',\n",
       "  'start': 15,\n",
       "  'end': 17},\n",
       " {'entity': 'B-Ⓗ',\n",
       "  'score': 0.99697304,\n",
       "  'index': 5,\n",
       "  'word': 'fi',\n",
       "  'start': 18,\n",
       "  'end': 20},\n",
       " {'entity': 'B-Ⓗ',\n",
       "  'score': 0.99508345,\n",
       "  'index': 6,\n",
       "  'word': '##ls',\n",
       "  'start': 20,\n",
       "  'end': 22},\n",
       " {'entity': 'B-Ⓑ',\n",
       "  'score': 0.99535537,\n",
       "  'index': 7,\n",
       "  'word': '1927',\n",
       "  'start': 23,\n",
       "  'end': 27},\n",
       " {'entity': 'B-Ⓚ',\n",
       "  'score': 0.99724364,\n",
       "  'index': 8,\n",
       "  'word': 'francais',\n",
       "  'start': 28,\n",
       "  'end': 36},\n",
       " {'entity': 'B-Ⓘ',\n",
       "  'score': 0.9956045,\n",
       "  'index': 9,\n",
       "  'word': 'id',\n",
       "  'start': 37,\n",
       "  'end': 39},\n",
       " {'entity': 'B-Ⓘ',\n",
       "  'score': 0.5931944,\n",
       "  'index': 10,\n",
       "  'word': '##em',\n",
       "  'start': 39,\n",
       "  'end': 41}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
